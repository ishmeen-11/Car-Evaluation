# ğŸš€ Decision Tree Classification - Car Evaluation Dataset

## ğŸ“Œ Project Overview
This project is part of my **30-day data science challenge** where I complete one project per day using different machine learning models. On **Day 3**, I implemented a **Decision Tree Classifier** on the **Car Evaluation dataset** to predict car acceptability based on various categorical features.

## ğŸ“‚ Dataset
- **Source**: Car Evaluation Dataset[https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set]
- **Description**: The dataset consists of **1,727 instances** with **6 categorical features**:
  - **buying** ğŸ’°: Cost of buying the car (vhigh, high, med, low)
  - **maint** ğŸ”§: Maintenance cost (vhigh, high, med, low)
  - **doors** ğŸšª: Number of doors (2, 3, 4, 5more)
  - **persons** ğŸ§‘â€ğŸ¤â€ğŸ§‘: Number of persons it can hold (2, 4, more)
  - **lug_boot** ğŸ’: Size of luggage boot (small, med, big)
  - **safety** âš ï¸: Safety rating (low, med, high)
- **Target Variable**:
  - âœ… unacc (Unacceptable)
  - âœ… acc (Acceptable)
  - âœ… good (Good)
  - âœ… vgood (Very Good)

## ğŸš€ Technologies Used
- **Python** ğŸ
- **Pandas & NumPy** ğŸ“Š â€“ Data handling and preprocessing
- **Scikit-Learn** ğŸ¤– â€“ Machine Learning (Decision Tree Classifier)
- **Matplotlib & Seaborn** ğŸ“ˆ â€“ Data visualization
- **Graphviz** ğŸŒ³ â€“ Decision Tree visualization

## ğŸ“œ Project Workflow
### 1ï¸âƒ£ Data Preprocessing
âœ… Loaded and explored the dataset
âœ… Checked for missing values and performed data cleaning
âœ… Converted categorical features into numerical using **Ordinal Encoding**
âœ… Split the dataset into **training (67%)** and **testing (33%)** sets

### 2ï¸âƒ£ Model Training & Evaluation
âœ” Implemented **Decision Tree Classifier** using `sklearn.tree.DecisionTreeClassifier`
âœ” Used two splitting criteria:
   - **Gini Index** ğŸ²
   - **Entropy (Information Gain)** ğŸ“Š
âœ” Trained models with **max_depth=3** to prevent overfitting
âœ” Evaluated model accuracy:
  - **Gini Index Accuracy**: **80.53%**
  - **Entropy Accuracy**: **80.53%**
âœ” Visualized decision trees using `graphviz` and `tree.plot_tree`
âœ” Compared training and test accuracy to check for overfitting

## âš¡ Results & Insights
ğŸ”¹ The Decision Tree classifier achieved **80.5% accuracy**, showing good performance on categorical data.
ğŸ”¹ Both **Gini Index** and **Entropy** provided the same accuracy in this case.
ğŸ”¹ Decision Trees are easy to interpret and visualize, making them useful for classification problems with categorical variables.

## ğŸ”¥ Next Steps
ğŸš€ Implement **pruning techniques** to optimize the decision tree.
ğŸš€ Perform **hyperparameter tuning** (`max_depth`, `min_samples_split`).
ğŸš€ Compare Decision Tree with **other classifiers** like **Random Forest** and **SVM**.

---
ğŸ› ï¸ **This is part of my 30-day challenge where I explore different ML models daily. Stay tuned for Day 4!** ğŸ¯

ğŸ“Œ **Author**
Ishmeen Garewal

